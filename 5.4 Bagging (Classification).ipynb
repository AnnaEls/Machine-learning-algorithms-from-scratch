{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6893f290",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3638019b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#***************************************************************************************************************\n",
    "class MyLogReg():\n",
    "    \n",
    "      \n",
    "    def __init__(self, n_iter=10, learning_rate=0.1, w=None, metric=None,reg = None, l1_coef=0,l2_coef=0,sgd_sample = None, random_state=42): #cla ss initialization\n",
    "        self.n_iter= n_iter\n",
    "        self.learning_rate = learning_rate\n",
    "        self.w = w\n",
    "        self.metric = metric\n",
    "        self.reg = reg\n",
    "        self.l1_coef = l1_coef\n",
    "        self.l2_coef = l2_coef\n",
    "        self.sgd_sample = sgd_sample\n",
    "        self.random_state = random_state\n",
    "     \n",
    "        \n",
    "    def __repr__(self):\n",
    "        return f'MyLogReg class: n_iter={self.n_iter}, learning_rate={self.learning_rate}'\n",
    "    \n",
    "    def fit(self, X, y, verbose=False):\n",
    "        eps = 1e-15\n",
    "        X['new'] = 1\n",
    "        col = X.pop(\"new\")\n",
    "        X.insert(0, col.name, col)\n",
    "        random.seed(self.random_state)\n",
    "        N_of_features = len(X.columns)\n",
    "        self.w = np.ones(N_of_features)\n",
    "        for i in range(int(self.n_iter)):               \n",
    "            y_ = 1/(1+np.exp(-np.dot(X,self.w)))\n",
    "            X_data,y_data = self.select_data(X,y)\n",
    "            y_pred = 1/(1+np.exp(-np.dot(X_data,self.w)))\n",
    "            LogLoss = -(y*np.log(y_+eps)+(1-y)*(1-np.log(1-y_+eps))).mean() + self.reg_()[0]\n",
    "            grad_Log = (1/len(y_data))*np.dot(np.subtract(y_pred,y_data),X_data) + self.reg_()[1]\n",
    "            if isinstance(self.learning_rate, float)== True:\n",
    "                self.w = self.w - self.learning_rate*grad_Log\n",
    "            else:\n",
    "                self.w = self.w - self.learning_rate(i+1)*grad_Log\n",
    "            if (verbose != False): \n",
    "                if (i%verbose ==0):\n",
    "                    print(f'{i}| loss:{LogLoss}')\n",
    "        \n",
    "                \n",
    "    def get_coef(self):\n",
    "        \n",
    "        return self.w[1:]\n",
    "    \n",
    "    def predict_proba(self,X):\n",
    "        X['new'] = 1\n",
    "        col = X.pop(\"new\")\n",
    "        X.insert(0, col.name, col)\n",
    "        return 1/(1+np.exp(-np.dot(X,self.w)))\n",
    "    \n",
    "    def predict(self,X):\n",
    "        X['new'] = 1\n",
    "        col = X.pop(\"new\")\n",
    "        X.insert(0, col.name, col)\n",
    "        P = 1/(1+np.exp(-np.dot(X,self.w)))\n",
    "        classes = np.zeros(P.shape,dtype=np.int)\n",
    "        classes[np.where(P > 0.5)]=1\n",
    "        return classes\n",
    "    \n",
    "    def get_best_score(self):\n",
    "        y_classes = self.predict(X)\n",
    "        TP=np.count_nonzero((y==1)&(y_classes==1))\n",
    "        TN = np.count_nonzero((y==0)&(y_classes==0))\n",
    "        FP = np.count_nonzero((y==0)&(y_classes==1))\n",
    "        FN = np.count_nonzero((y==1)&(y_classes==0))\n",
    "        return self.calculate_metric(TP, TN, FP, FN)\n",
    "    \n",
    "    def calculate_metric(self, TP, TN, FP, FN, beta = 1):\n",
    "        global y\n",
    "        if self.metric == 'accuracy':\n",
    "            return (TP+TN)/(TP+TN+FP+FN)\n",
    "        elif self.metric == 'precision':\n",
    "            return TP/(TP+FP)\n",
    "        elif self.metric == 'recall':\n",
    "            return TP/(TP+FN)\n",
    "        elif self.metric == 'f1':\n",
    "            pres = TP/(TP+FP)\n",
    "            rec = TP/(TP+FN)\n",
    "            return (1+np.square(beta))*pres*rec/(np.square(beta)*pres + rec)\n",
    "        elif self.metric == 'roc_auc':\n",
    "            probs = self.predict_proba(X)\n",
    "            sorted_idx = np.argsort(-probs)\n",
    "            probs_sorted = probs[sorted_idx]\n",
    "            y_sorted = y[sorted_idx]\n",
    "            \n",
    "            sum=0.\n",
    "            P = len(np.where(y==1)[0])\n",
    "            N = len(np.where(y==0)[0])\n",
    "            \n",
    "            \n",
    "            for prob, class_ in zip(probs_sorted,y_sorted):\n",
    "                if class_ == 0:\n",
    "                    sum = sum + len(np.where(y_sorted[probs_sorted > prob]==1)[0])\n",
    "                    sum = sum + 0.5*len(np.where(y_sorted[probs_sorted == prob]==1)[0])\n",
    "            return sum/(P*N)       \n",
    "        \n",
    "    def  reg_(self):\n",
    "        reg_loss = 0\n",
    "        reg_grad = 0\n",
    "        if self.reg == 'l1':\n",
    "            reg_loss = self.l1_coef*np.sum(np.abs(self.w))\n",
    "            reg_grad = self.l1_coef*np.sign(self.w)\n",
    "        elif self.reg == 'l2':\n",
    "            reg_loss = self.l2_coef*np.sum(np.square(self.w))\n",
    "            reg_grad = 2*self.l2_coef*self.w\n",
    "        elif self.reg == 'elasticnet':\n",
    "            reg_loss = self.l1_coef*np.sum(np.abs(self.w))+self.l2_coef*np.sum(np.square(self.w))\n",
    "            reg_grad = self.l1_coef*np.sign(self.w)+2*self.l2_coef*self.w  \n",
    "        return reg_loss, reg_grad\n",
    "    \n",
    "    def select_data(self,X,y):\n",
    "        if self.sgd_sample == None:\n",
    "            return X, y\n",
    "        elif isinstance(sgd_sample,int) == True:\n",
    "            sample_rows_idx = random.sample(range(X.shape[0]), self.sgd_sample)\n",
    "            return X.iloc[sample_rows_idx,:], y.iloc[sample_rows_idx]\n",
    "        else:\n",
    "            sample_rows_idx = random.sample(range(X.shape[0]), int(self.sgd_sample*X.shape[0]))\n",
    "            return X.iloc[sample_rows_idx,:], y.iloc[sample_rows_idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f35360af",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node():\n",
    "    \n",
    "    def __init__(self, node_type = 'Node', side=None, feature = None, split_value = None, parent=None, *, node_depth = None,  ID = None, value = None, children=[]): #class initialization\n",
    "        self.node_type = node_type #Leaf or Node        \n",
    "        self.side = side #LEft or Right\n",
    "        self.feature = feature\n",
    "        self.split_value = split_value\n",
    "        self.parent = parent\n",
    "        \n",
    "        self.node_depth = node_depth \n",
    "        self.ID = ID\n",
    "        self.value = value #only for Leaf  \n",
    "        self.children = []\n",
    "        \n",
    "        self.Np = 0 #number of points in the node\n",
    "        \n",
    "            \n",
    "        \n",
    "        \n",
    "    def add_child(self, new_value):\n",
    "        self.children.append(new_value)\n",
    "#*********************************************************************************************************************\n",
    "#*********************************************************************************************************************\n",
    "\n",
    "class MyTreeClf():\n",
    "    \n",
    "    def __init__(self, max_depth = 5,  min_samples_split =2, max_leafs = 20, bins = None, criterion = 'entropy'): \n",
    "        self.max_depth = max_depth #maximum possible depth of tree\n",
    "        self.min_samples_split = min_samples_split #minimum sample split\n",
    "        self.max_leafs = max_leafs #maximum possible number of leaves in a tree\n",
    "        self.bins = bins\n",
    "        \n",
    "        #tree parameters\n",
    "        self.tree = []\n",
    "        self.leafs_cnt = 0 #number of created leaves in the tree\n",
    "        self.potential_leafs_cnt = 1 #counting potential leaves\n",
    "        self.leafs_sum = 0 #sum of the leaves values\n",
    "        \n",
    "        self.histogram = {}\n",
    "        \n",
    "        self.criterion = criterion\n",
    "        \n",
    "        self.fi = {}\n",
    "        \n",
    "        self.N = 0\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return f'MyTreeClf class: max_depth={self.max_depth}, min_samples_split={self.min_samples_split}, max_leafs={self.max_leafs}'\n",
    "    \n",
    "    #------------------------------------------------------------------------------------------------------------\n",
    "    def fit(self, X, y): #receives panda dataframe and series\n",
    "        if self.bins != None:\n",
    "            for feature in X.columns:\n",
    "                self.histogram.update({feature: self.get_hist_delimeters(X[feature].values)})\n",
    "                \n",
    "        for feature in X.columns:\n",
    "            self.fi.update({feature: 0})\n",
    "        \n",
    "        self.N = len(y.values)\n",
    "        \n",
    "        #Create root node\n",
    "        feature, split_value, ig = self.get_best_split(X,y)\n",
    "        X_left, y_left, X_right, y_right = self.split_dataframe(X, y, feature, split_value)\n",
    "        if ig == 0.0 or len(y_left) == 0 or len(y_right) == 0:            \n",
    "            print('All targets belong to class:', np.sum(y.values)/len(y.values) )\n",
    "        else:\n",
    "            _node = self.register_Node(\"Node\", None, feature, split_value, None, y)\n",
    "            #--------feature importance update---\n",
    "            _node.Np = len(y.values)\n",
    "            self.update_fi(y, _node, None)\n",
    "            #----------------------------------\n",
    "            self.grow_tree(X_left, y_left, 'Left', _node)\n",
    "            self.grow_tree(X_right, y_right, 'Right', _node)\n",
    "            \n",
    "        \n",
    "    def grow_tree(self, X, y, side, parent): #receives panda dataframe and series, string and Node\n",
    "        feature, split_value, ig = self.get_best_split(X,y)\n",
    "        X_left, y_left, X_right, y_right = self.split_dataframe(X, y, feature, split_value)\n",
    "        \n",
    "        if ig != 0.0 and len(y_left) != 0 and len(y_right) != 0 and (parent.node_depth < self.max_depth) and (len(y.values) >= self.min_samples_split) and (self.leafs_cnt + self.potential_leafs_cnt < self.max_leafs):\n",
    "            _node = self.register_Node('Node', side, feature, split_value, parent, y)\n",
    "            #--------feature importance update---\n",
    "            _node.Np = len(y.values)\n",
    "            self.update_fi(y, _node, parent)\n",
    "            #----------------------------------\n",
    "            self.grow_tree(X_left, y_left, 'Left', _node)\n",
    "            self.grow_tree(X_right, y_right, 'Right', _node)\n",
    "        else:\n",
    "            _node = self.register_Node('Leaf', side, feature, split_value, parent, y)\n",
    "            #--------feature importance update---\n",
    "            _node.Np = len(y.values)\n",
    "            self.update_fi(y, _node, parent)\n",
    "            #----------------------------------\n",
    "            return _node\n",
    "            \n",
    "    #-------------------------------------------------------------------------------------------------------------\n",
    "    #-------------------------------------------------------------------------------------------------------------\n",
    "    def register_Node(self, node_type, side, feature, split_value, parent, y):\n",
    "        #1.setting node depth\n",
    "        if parent != None:\n",
    "            node_depth = parent.node_depth +1\n",
    "        else:\n",
    "            node_depth = 1\n",
    "            \n",
    "        #2.setting node ID\n",
    "        if node_type == 'Node':\n",
    "            if side == 'Left':\n",
    "                 ID = parent.ID + '.1' \n",
    "            elif side == 'Right':\n",
    "                 ID = parent.ID + '.2'\n",
    "            else:\n",
    "                ID = '1'\n",
    "        \n",
    "        if node_type == 'Leaf':\n",
    "            ID = parent.ID\n",
    "            \n",
    "        #3.Setting node value\n",
    "        if node_type == 'Leaf':\n",
    "            value = np.sum(y.values)/len(y.values)\n",
    "        else:\n",
    "            value = None\n",
    "        \n",
    "        new_node = Node(node_type, side, feature, split_value, parent, node_depth=node_depth, ID=ID, value=value)\n",
    "        self.tree.append(new_node)\n",
    "        \n",
    "        #add as a child to parent node\n",
    "        if parent != None :                            \n",
    "            parent.add_child(new_node)\n",
    "            \n",
    "        #update counts\n",
    "        if node_type == \"Node\":\n",
    "            self.potential_leafs_cnt = self.potential_leafs_cnt + 1\n",
    "        elif node_type == \"Leaf\":\n",
    "            self.leafs_cnt = self.leafs_cnt + 1\n",
    "            self.potential_leafs_cnt = self.potential_leafs_cnt - 1\n",
    "            self.leafs_sum = self.leafs_sum + value\n",
    "        return new_node\n",
    "        \n",
    "            \n",
    "    def print_tree_full(self):\n",
    "        for node in self.tree:\n",
    "                print(node.__dict__)\n",
    "    \n",
    "    def print_tree(self):\n",
    "        for node in self.tree:\n",
    "            if node.node_type == 'Node':\n",
    "                \n",
    "                print(node.__dict__['ID'], node.__dict__['feature'], '>', node.__dict__['split_value'])\n",
    "            else:\n",
    "                print(node.__dict__['ID'],node.__dict__['side'], '-', node.__dict__['value'])\n",
    "                \n",
    "    def move_up_the_tree(self, X, _node,i):\n",
    "        if _node.node_type =='Leaf':\n",
    "            self.predictions[i]=float(_node.value)\n",
    "        elif _node.node_type =='Node': \n",
    "            if X[_node.feature] <= _node.split_value:\n",
    "                if _node.children[0].side =='Left':\n",
    "                    self.move_up_the_tree(X, _node.children[0],i)\n",
    "                else: self.move_up_the_tree(X, _node.children[1],i)\n",
    "               \n",
    "            else:\n",
    "                if _node.children[0].side =='Right':\n",
    "                    self.move_up_the_tree(X, _node.children[0],i)\n",
    "                else: self.move_up_the_tree(X, _node.children[1],i)\n",
    "    #-------------------------------------------------------------------------------------------------------------\n",
    "    #-------------------------------------------------------------------------------------------------------------\n",
    "    def predict_proba(self,X):\n",
    "        self.predictions = np.zeros(X.shape[0])\n",
    "        for i in range(X.shape[0]):\n",
    "            self.move_up_the_tree(X.iloc[i,:], self.tree[0],i)\n",
    "        return(self.predictions)\n",
    "            \n",
    "    def predict(self,X):\n",
    "        self.predict_proba(X)\n",
    "        return (self.predictions > 0.5)*1 \n",
    "    #-------------------------------------------------------------------------------------------------------------\n",
    "    #-------------------------------------------------------------------------------------------------------------\n",
    "    #Calculate the best split\n",
    "    def gini(self, y):\n",
    "        p1 = len(y[y == 0])/len(y)\n",
    "        p2 = len(y[y == 1])/len(y)\n",
    "        return 1 - p1**2 - p2**2\n",
    "    \n",
    "    def entropy(self, y): # receives 1D numpy array\n",
    "        p1 = len(y[y == 0])/len(y)\n",
    "        p2 = len(y[y == 1])/len(y)\n",
    "        if p1 == 0 or p2 == 0:\n",
    "            return 0.0\n",
    "        else:\n",
    "            return -p1*np.log2(p1) - p2*np.log2(p2)\n",
    "\n",
    "    def data_split(self, X, y, threshold): #receives two 1D numpy arrays and a float\n",
    "        X_left = X[X <= threshold]\n",
    "        y_left = y[X <= threshold]\n",
    "        X_right = X[X > threshold]\n",
    "        y_right = y[X > threshold]\n",
    "        return X_left, y_left, X_right, y_right\n",
    "    \n",
    "    def get_IG(self, X, y, threshold): #receives two numpy arrays and a float\n",
    "        #split the data by the threshold\n",
    "        _, y_left, __, y_right = self.data_split(X, y, threshold)\n",
    "        if len(y_left) == 0 or len(y_right) == 0: #threshold does not split the data\n",
    "            return 0.0\n",
    "        else:\n",
    "            if self.criterion == 'entropy':\n",
    "                S0 = self.entropy(y)\n",
    "                S1 = self.entropy(y_left)*len(y_left)/len(y)\n",
    "                S2 = self.entropy(y_right)*len(y_right)/len(y)\n",
    "                IG = S0 - S1 -S2\n",
    "            elif self.criterion == 'gini':\n",
    "                Gp = self.gini(y)\n",
    "                Gl = self.gini(y_left)*len(y_left)/len(y)\n",
    "                Gr = self.gini(y_right)*len(y_right)/len(y)\n",
    "                IG = Gp - Gl - Gr\n",
    "            return IG\n",
    "        \n",
    "    def get_native_delimeters(self, X): #receives 1D numpy array\n",
    "        X_unique = np.unique(np.sort(X))\n",
    "        native_delimeters = [np.mean([X_unique[i-1], X_unique[i]]) for i in range(1, len(X_unique))]\n",
    "        return native_delimeters\n",
    "    \n",
    "    def get_hist_delimeters(self, X): #receives 1D numpy array\n",
    "        hist_delimeters = np.histogram(X, self.bins)[1][1:-1]\n",
    "        return hist_delimeters\n",
    "    \n",
    "    def get_best_split(self, X, y): #receives panda dataframe and panda series\n",
    "        feature_best_split = {}\n",
    "        for feature in X.columns:\n",
    "            if len(X[feature].values) == 0 or np.max(X[feature].values) == np.min(X[feature].values):\n",
    "                feature_best_split.update({feature: [None, 0.0]}) #feature has no values or any delimeters\n",
    "            else:\n",
    "                if self.bins == None:                    \n",
    "                    feature_delimeters = self.get_native_delimeters(X[feature].values)\n",
    "                else: \n",
    "                    X_unique = np.unique(np.sort(X))\n",
    "                    if len(X_unique) <= self.bins:\n",
    "                        feature_delimeters = self.get_native_delimeters(X[feature].values)\n",
    "                    else:\n",
    "                        feature_delimeters = self.histogram[feature]\n",
    "                    \n",
    "                feature_igs = [self.get_IG(X[feature].values, y.values, feature_delimeters[i]) for i in range(len(feature_delimeters))]\n",
    "                feature_best_split.update({feature: [feature_delimeters[np.argmax(feature_igs)],np.max(feature_igs)]})\n",
    "        \n",
    "        split_value, ig   = max(feature_best_split.values(), key=lambda x: x[1])\n",
    "        feature = next(k for k, v in feature_best_split.items() if v == [split_value, ig])\n",
    "        return feature, split_value, ig\n",
    "    #-------------------------------------------------------------------------------------------------------------\n",
    "        \n",
    "    def split_dataframe(self, X, y, feature, threshold): #X,y - np.arrays, threshold - float\n",
    "        X_left = X[X[feature] <= threshold].reset_index(drop = True)\n",
    "        y_left = y[X[feature] <= threshold].reset_index(drop = True)\n",
    "        X_right = X[X[feature] > threshold].reset_index(drop = True)\n",
    "        y_right = y[X[feature] > threshold].reset_index(drop = True)\n",
    "        return X_left, y_left, X_right, y_right\n",
    "    \n",
    "    def update_fi(self, y, _node, parent):\n",
    "        if _node.node_type == 'Node':\n",
    "            if self.criterion == 'entropy':\n",
    "                FI = self.entropy(y.values)*_node.Np/self.N\n",
    "            elif self.criterion == 'gini':\n",
    "                FI = self.gini(y.values)*_node.Np/self.N\n",
    "            self.fi.update({_node.feature: self.fi[_node.feature] + FI })\n",
    "        \n",
    "        if parent != None:\n",
    "            if self.criterion == 'entropy':\n",
    "                FI = self.entropy(y.values)*_node.Np/self.N\n",
    "            elif self.criterion == 'gini':\n",
    "                FI = self.gini(y.values)*_node.Np/self.N\n",
    "            self.fi.update({parent.feature: self.fi[parent.feature] - FI })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e7962d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyKNNClf():\n",
    "    \n",
    "    def __init__(self, k=3, metric = 'euclidean', weight = 'uniform'): #class initialization\n",
    "        self.k = k\n",
    "        self.train_size = None\n",
    "        self.X_train = None\n",
    "        self.y_train = None\n",
    "        self.metric = metric\n",
    "        self.weight = weight\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return f'MyKNNClf class: k={self.k}'\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.X_train = X.copy()\n",
    "        self.y_train = y.copy()\n",
    "        self.train_size = X.shape\n",
    "        \n",
    "    def euclidian(self, Xtest, Xtrain):\n",
    "        return np.sqrt(np.sum((Xtest[:,np.newaxis]-Xtrain)**2, axis=2))\n",
    "    \n",
    "    def chebyshev(self, Xtest, Xtrain):\n",
    "        return np.max(np.abs(Xtest[:,np.newaxis]-Xtrain), axis=2)\n",
    "    \n",
    "    def manhattan(self, Xtest, Xtrain):\n",
    "        return np.sum(np.abs(Xtest[:,np.newaxis]-Xtrain), axis=2)\n",
    "    \n",
    "    def cosine(self, Xtest, Xtrain):\n",
    "        Xtest_norm = np.linalg.norm(Xtest, axis=1)\n",
    "        Xtrain_norm = np.linalg.norm(Xtrain, axis=1)\n",
    "        return 1 - np.dot(Xtest, Xtrain.T)/(Xtest_norm[:,np.newaxis]*Xtrain_norm)\n",
    "    \n",
    "    def calculate_metric(self,X):\n",
    "        if self.metric == 'chebyshev':\n",
    "            return self.chebyshev(X.values, self.X_train.values)\n",
    "        if self.metric == 'manhattan':\n",
    "            return self.manhattan(X.values, self.X_train.values)\n",
    "        if self.metric == 'cosine':\n",
    "            return self.cosine(X.values, self.X_train.values)\n",
    "        if self.metric == 'euclidean':\n",
    "            return self.euclidian(X.values, self.X_train.values)\n",
    "        \n",
    "    def get_rank(self,X):\n",
    "        D = self.calculate_metric(X)\n",
    "        min_k = self.y_train.values[np.argsort(D,axis=1)][:,:self.k]\n",
    "        cl = np.array(list(set(self.y_train)))\n",
    "        N_cl =  len(cl)\n",
    "        sum_ = np.sum(1/(np.arange(self.k)+1)) \n",
    "        Q = np.zeros([min_k.shape[0],N_cl])\n",
    "        for j in range(N_cl):\n",
    "            for i in range(min_k.shape[0]):\n",
    "                Q[i,j] = np.sum(1/(np.array(np.where(min_k[i,:] == j))+1))/sum_\n",
    "        return Q\n",
    "    \n",
    "    def get_dist(self,X):\n",
    "        D = self.calculate_metric(X)\n",
    "        min_k = self.y_train.values[np.argsort(D,axis=1)][:,:self.k]\n",
    "        D_min_k = (np.sort(D,axis=1))[:,:self.k]     \n",
    "        cl = np.array(list(set(self.y_train)))\n",
    "        N_cl =  len(cl)\n",
    "        sum_ = np.sum(1/D_min_k, axis=1)\n",
    "        \n",
    "        Q = np.zeros([min_k.shape[0],N_cl])\n",
    "        for i in range(min_k.shape[0]):\n",
    "            for j in range(N_cl):            \n",
    "                Q[i,j] = np.sum(1/D_min_k[i,:].take(np.where(min_k[i,:] == j)))/np.sum((1/D_min_k)[i,:])\n",
    "            #print(f'{Q[i,:]} and sum:{(1/D_min_k)[i,:]} and{min_k[i,:]}:{np.sum((1/D_min_k)[i,:])} ')            \n",
    "        \n",
    "        return Q\n",
    "        \n",
    "        \n",
    "        \n",
    "    def predict(self,X):\n",
    "        if self.weight == 'uniform':\n",
    "            P = self.predict_proba(X)\n",
    "            return (P >= 0.5)*1\n",
    "        \n",
    "        elif self.weight == 'rank':\n",
    "            Q = self.get_rank(X)\n",
    "            return np.argmax(Q, axis = 1)\n",
    "        \n",
    "        elif self.weight == 'distance':\n",
    "            Q = self.get_dist(X)\n",
    "            return np.argmax(Q, axis = 1)\n",
    "            \n",
    "    \n",
    "    def predict_proba(self,X): \n",
    "       \n",
    "        \n",
    "        if self.weight == 'uniform':\n",
    "            D = self.calculate_metric(X)\n",
    "            min_k = self.y_train.values[np.argsort(D,axis=1)][:,:self.k]\n",
    "            return np.sum(min_k, axis=1)/self.k\n",
    "        \n",
    "        elif self.weight == 'rank':\n",
    "            Q = self.get_rank(X)\n",
    "            return Q[:,1]\n",
    "        \n",
    "        elif self.weight == 'distance':\n",
    "            Q = self.get_dist(X)\n",
    "            return Q[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a264e752",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyBaggingClf():\n",
    "    \n",
    "    def __init__(self, estimator=None, n_estimators=0.5, max_samples=0.5, random_state=42, oob_score = None): #cla ss initialization\n",
    "        self.estimator= estimator\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_samples = max_samples\n",
    "        self.random_state = random_state\n",
    "        self.estimators = [None]*self.n_estimators\n",
    "        \n",
    "        self.oob_score = oob_score\n",
    "        self.oob_score_ = 0\n",
    "        self.OOB_predictions = {}\n",
    "        \n",
    "     \n",
    "    def __repr__(self):\n",
    "        return f'MyBaggingClf class: estimator={self.estimator}, n_estimators={self.n_estimators}, max_samples={self.max_samples}, random_state={self.random_state}'\n",
    "    \n",
    "    \n",
    "    def  fit(self, X, y): #received pandas dataframe and series\n",
    "        if self.oob_score != None:\n",
    "            for i in range(X.shape[0]):\n",
    "                self.OOB_predictions.update({str(i): np.zeros(0)})\n",
    "        \n",
    "        \n",
    "        random.seed(self.random_state)\n",
    "        rows_smpl_cnt = int(np.round(self.max_samples*X.shape[0],0))\n",
    "        rows_list = np.zeros([rows_smpl_cnt, self.n_estimators])\n",
    "        for i in range(self.n_estimators):\n",
    "            rows_list[:,i] = random.choices(list(X.index), k=rows_smpl_cnt)\n",
    "            \n",
    "        for i in range(self.n_estimators):\n",
    "            self.estimators[i] = copy.deepcopy(self.estimator)\n",
    "        \n",
    "        for i in range(self.n_estimators):\n",
    "            if self.estimator.__class__.__name__ == 'MyLogReg':\n",
    "                self.estimators[i].fit(X.loc[rows_list[:,i]],y.loc[rows_list[:,i]],False)\n",
    "            else:\n",
    "                self.estimators[i].fit(X.loc[rows_list[:,i]],y.loc[rows_list[:,i]])\n",
    "                \n",
    "            if self.oob_score != None:\n",
    "                X_oob = X.drop(rows_list[:,i])\n",
    "                y_oob = y.drop(rows_list[:,i]).reset_index()\n",
    "                predictions = self.estimators[i].predict_proba(X_oob)\n",
    "                for i in range(y_oob.shape[0]):\n",
    "                    temp = np.append(self.OOB_predictions[str(y_oob['index'][i])],predictions[i])\n",
    "                    self.OOB_predictions.update({str(y_oob['index'][i]): temp })\n",
    "                \n",
    "                \n",
    "        if self.oob_score != None:\n",
    "            y1 = np.zeros(0)\n",
    "            y2 = np.zeros(0)\n",
    "            scores = np.zeros(0)\n",
    "            for index, array in self.OOB_predictions.items():\n",
    "                if len(array) != 0:\n",
    "                    y1=np.append(y1, y.values[int(index)]) #actual values\n",
    "                    y2=np.append(y2, (np.mean(array) > 0.5)*1) #prediction\n",
    "                    if self.oob_score == 'roc_auc':\n",
    "                        scores=np.append(scores, np.mean(array))\n",
    "                        \n",
    "                \n",
    "            TP=np.count_nonzero((y1==1)&(y2==1))\n",
    "            TN = np.count_nonzero((y1==0)&(y2==0))\n",
    "            FP = np.count_nonzero((y1==0)&(y2==1))\n",
    "            FN = np.count_nonzero((y1==1)&(y2==0))\n",
    "            \n",
    "            \n",
    "            self.calculate_metric(TP, TN, FP, FN, y1, scores)\n",
    "                \n",
    "                \n",
    "    def  predict_proba(self, X):\n",
    "        predictions = np.zeros(X.shape[0])\n",
    "        for i in range(len(self.estimators)):\n",
    "            predictions = predictions +  self.estimators[i].predict_proba(X)\n",
    "        return predictions/self.n_estimators\n",
    "            \n",
    "    def predict_class(self, X):\n",
    "        class_labels = np.zeros([X.shape[0],len(self.estimators)])\n",
    "        classes = np.zeros(X.shape[0])\n",
    "        for i in range(len(self.estimators)):\n",
    "            predictions = self.estimators[i].predict_proba(X)\n",
    "            class_labels[:,i] = (predictions > 0.5)*1\n",
    "        \n",
    "        for k in range(X.shape[0]):\n",
    "            cl = np.unique(class_labels[k,:], return_counts=True)\n",
    "            classes[k] = np.round(cl[0][np.argmax(cl[1])],0)\n",
    "        classes = classes.astype(int)\n",
    "        return classes\n",
    "        \n",
    "    def predict(self,X,type):\n",
    "         if type == 'mean':\n",
    "            predictions = self.predict_proba(X)\n",
    "            return (predictions > 0.5)*1\n",
    "         elif type == 'vote':\n",
    "            classes = self.predict_class(X)\n",
    "            return classes\n",
    "        \n",
    "        \n",
    "    def calculate_metric(self, TP, TN, FP, FN, y_, probs, beta = 1):\n",
    "        if self.oob_score == 'accuracy':\n",
    "            self.oob_score_ = (TP+TN)/(TP+TN+FP+FN)\n",
    "        elif self.oob_score == 'precision':\n",
    "            self.oob_score_ =  TP/(TP+FP)\n",
    "        elif self.oob_score == 'recall':\n",
    "            self.oob_score_ =  TP/(TP+FN)\n",
    "        elif self.oob_score == 'f1':\n",
    "            pres = TP/(TP+FP)\n",
    "            rec = TP/(TP+FN)\n",
    "            self.oob_score_ =  (1+np.square(beta))*pres*rec/(np.square(beta)*pres + rec)\n",
    "        elif self.oob_score  == 'roc_auc':\n",
    "            probs = np.round(probs,10)\n",
    "            sorted_idx = np.argsort(-probs)\n",
    "            probs_sorted = probs[sorted_idx]\n",
    "            y_sorted = y_[sorted_idx]\n",
    "            \n",
    "            sum=0.\n",
    "            P = len(np.where(y_sorted==1)[0])\n",
    "            N = len(np.where(y_sorted==0)[0])\n",
    "            \n",
    "            \n",
    "            for prob, class_ in zip(probs_sorted,y_sorted):\n",
    "                if class_ == 0.:\n",
    "                    sum = sum + len(np.where(y_sorted[probs_sorted > prob]==1)[0])\n",
    "                    sum = sum + 0.5*len(np.where(y_sorted[probs_sorted == prob]==1)[0])\n",
    "            self.oob_score_ = sum/(P*N) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
